{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec248b8c-fee0-472f-8a7c-e9d65b4be936",
   "metadata": {},
   "source": [
    "# Example 1: Sinatra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e01f255-9398-4a55-af3c-f9a0c6132534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_utils import make_task, transcribe_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1566d81b-d5a5-473c-8683-2dd96924bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.youtube.com/watch?v=ZEcqHA7dbwM'\n",
    "model_type = 'tiny'\n",
    "transcription_task = make_task(url, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d55424-a17f-4eac-aa2d-dce76bec3d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tiny model...\n",
      "Model loaded succssfully...\n",
      "Transcribing ./youtube-flow-audio-files/fly_me_to_the_moon_2008_remastered...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddie/miniconda3/envs/youtube-transcription/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Let me play among the stars. Let me see what spring is like on. As you put a mask, in other words, hold my hand. In other words, baby kiss me. Fill my heart with song and let me sing forevermore. You are all I long for, all I worship and the dawn. In other words, please be true. In other words, I love you. Fill my heart with song and let me sing forevermore. You are all I long for, all I worship and the dawn. In other words, please be true. In other words, in other words, I love you. You.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription = transcribe_video(transcription_task)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672e55f-163d-4f43-bd2e-c6ed0fcbc934",
   "metadata": {},
   "source": [
    "# Example 2: Fireside Chat #1\n",
    "* Video Time: 02:21:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "361a312a-a8c3-4645-80ce-fe02d1294396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tiny model...\n",
      "Model loaded succssfully...\n",
      "Transcribing ./youtube-flow-audio-files/fireside_chat_1_how_to_produce_sustainable_business_value_with_machine_learning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddie/miniconda3/envs/youtube-transcription/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 44s, sys: 1min 13s, total: 5min 57s\n",
      "Wall time: 3min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" you you chat, very excited to be here today with Hillary Parker talking about how to produce sustainable business value with machine learning and we'll get started in a few minutes. So in the meantime, please do introduce yourself in the YouTube chat. Let us know where you're watching from, what you're interested in with your Spectrum Machine Learning and Data Power software more generally and we'll get started in a few minutes. I feel like we need some elevated music or something, but my beat boxing skills aren't up to scratch. Hi everyone, it's Hugo Boundandison here from Out of Bounds. Welcome to our first fireside chat. Very excited to have Hillary Parker here today to talk about how to produce sustainable business value using machine learning. We'll get started in a couple of minutes. In the meantime, we'd love for you to introduce yourself in the chat, let us know where you're watching from, what you're interested in machine learning and product management on all of these things and we'll get started in a couple of minutes. All right, everybody. It is with great pleasure that I'm Hugo, I'm Anderson, by the way, ahead of developer relations and out of bounds. Today, I'm very excited to be speaking with Hillary Parker. I'll introduce in a second for us to chat about how to produce sustainable business value with machine learning. To that end, I'm going to stop sharing my screen and share my video on our last Hillary to do the same. Hey, there, Hillary. Hi. How are you? I'm good. How are you? Fantastic. Great to have you here today. Yeah, I was appreciating you saying your name because I think if I had always thought Bill and Anderson. And so I'm sure that's common. It's very common. I also get brown. The human mind will insert the R into it. Constantly that. Yeah, I just looked at it. There are so many problems with my name, including the hyphen because of a lot of databases, don't like hyphens, right? So when flying my ID rarely matches what airlines will, you know, one of one of many concerns of my the length as well. Also starting with a B means like people on Facebook when they create events, you'll be at the top of their list to be invited because it's ranked alphabet. Anyway, the trials and tribulations of a hyphenated name starting with a B, I would like to welcome everyone here today. This is very exciting for us to have our first fireside chat and it's such a pleasure to be doing so with with you, Hillary. I'll I'll get you to tell us a bit about your background, but I would like to let everyone know for those of you who don't know, Hillary, Hillary is a data scientist and independent consultant who's worked at Stitch Fix Etsy and has a data science product manager for the 2020 Biden for President campaign. You also have a podcast which you've been doing for many years now. I think six and a half now. So that's wild. It is called not so standard deviations. That's right. So anyone if you enjoy this this conversation, please do check out your Hillary and Roger Peng's podcast as well. Just a bit of housekeeping so to speak to start off this is a conversation a fireside chat. Feel free to ask any questions in the chat. We've got someone from Indonesia who's just joined and someone from Brazil as well, which is which is exciting. If you have any questions throughout, feel free to ask them in the chat. We may get to them during if not, you can ask them in the AMA on our community slack afterwards and I've placed places, placed at the link in the chat as well. And if you haven't heard of out of bounds, we're just really, we're just getting going now. So what we're really excited about out of bounds is we want to give data scientists as many powers as possible by making it easy for them to interact with modern infrastructure from scalable compute and data warehouses to orchestration systems. So we really want to where focused on building tools that allow data scientists to focus on what they want to while having access to all the infrastructure needs. We've been doing so through an open source framework called Metaflow, which my colleagues began an open source that Netflix and we're excited to build products around it now as well. So if that sounds interesting, definitely check us out. Without further ado, though, I'm really excited to start talking about best practices for PMing data and MLBowed software, what data scientists need to know about product management for data-powered products. I'm really excited to talk about file your modes. Someone just mentioned somebody in the fit just wrote their tuning in from Alpha Centauri. So welcome from the Cosmos. Also talking about how companies and data science functions in orgs can think about producing sustainable business value. So I've introduced you briefly, but I wonder whether you could just tell us a bit about your background, how you got started in the data world and your career so far with particular focus on data products and the stuff as well. Yeah, for sure. Yeah. So my academic background is sort of on the Straighten-Nero academic path through PhD. So I got a PhD from Hopkins School Public Health. And never was totally convinced I wanted to be professor or anything. I was the classic case of grad school's new college where I didn't know what to do. So I was like, I'll take grad school seems like a good idea. And so toward the end of it, like I said, I never had like coax and dreams of being a professor. So I was exploring, you know, all the options. Tech was really exciting to me. I was like a late adopter of tech to some degree. Like I wasn't very tech focused until grad school. And then, yeah, I reached out. So I was just talking about this on I did a Twitter space earlier today as well. And like the it's not generalizable, but I got very lucky reaching out to some key folks, including Hillary Mason, which people get us confused all the time. It's too Hillary. Yeah. So I just literally. A single L. Yes, that's okay. That's why it's confusing. Well, one of many reasons. Yeah, both a single L, both felt the right way. I love out in order to focus on names and spelling today as well. Yes. It's important. It's important. It really is. Hillary's are especially like into this. So if you ever meet a Hillary ask how many L's, like she'll be like, oh, I'm so glad you asked. Like, yeah, and then act like whichever one they say is right. So. But yeah, I reached out to her. She was like so generous with her time. And she was working a bit late at the time, which was just really cool stuff because they were essentially early days of like tracking the internet and like what does it mean for something to go viral? And they were, they had like this unique vantage point. So anyway, was convinced to go into tech, went into mid-D. York, went to Etsy. And that's where I picked up like product analytics. So I was essentially a product analyst there. And I worked under this amazing boss, Nell Thomas, who went on to become the CTO of the DNC Democratic National Committee. So you can see how this, how the how it started to progress. But yeah, so then quickly realized that product data science is very similar to biosetistics in terms of like experimentation and measuring impact and effect sizes and everything. So then move from there to stitch fix where it was sort of a I was both doing product analytics. And I was able to focus more on like data product development. And so got to work a lot with like developing new product, sorry, new data streams or products and thinking more holistically about the problem. Um, as well as just collaborating on like, arts, as seen data streams. So like, what data do we need to collect in order to, um, like give clients the right clothing and style them. So for those of you who don't know stitch fix is like a personal styling and like kind of like online shopping experience. So anyway, um, then yeah, with the binding campaign, I focused more primarily on the like product, uh, like product management. I mean, I was there for two months. So everything is like in a campaign as very, you know, you're doing whatever is required at the time. Um, but basically, the as you can imagine in a presidential campaign, especially one that like I felt was extremely important for the health of the nation. Um, you really want to make sure that the numbers that you're producing as a central data science team are accurate that they're delivered reliably. Um, and those things end up having as much impact as the values themselves because if if you have a team that really wants to use numbers, but your infrastructure is constantly failing you like it kind of doesn't matter how good the numbers are as you can imagine. Um, and so that was a lot of, you know, how do we QA things, what do the processes look like, how do we ensure that we're delivering the right stuff? Um, and so yeah, since I've been consulting, been also took a lot of time off, which I highly recommend to everyone. And, uh, yeah. So I think I rambled about your question long enough. Well, that was all very, very good content. I also encourage people to take time off. I've started recently at out of bounds and had three months off beforehand, um, which was if very, very restorative, um, I like you've worked in a variety of different verticals. Um, I think, which the, the variance is interesting, um, because let's see we can think of as a classic tech company in a lot of ways, classic e-commerce, right? Stitch fix is a variation on on this. I, I wouldn't take this analogy too far, but I think of Stitch Fix in the same way as I think of um, Netflix, for example, which is not classically an industry, which is tech per, per say, by the company, which is deeply rooted in and actually both, and deeply have techers foundational, right? Like entering perhaps other other verticals, right? Yeah. Um, yeah, I mean, it's definitely, yeah, at C's of like the biggest difference, the what I thought you're going to say is that at C, you like don't own any physical inventory and it's just an online marketplace. So it really is in that sense, like a classic tech company. Yeah. For Stitch Fix, like you actually we own like, I guess they own the inventory. And so there's like the risks of a traditional, like, you know, department store. And so as a result, the company is structured differently. And like there's just more irons in the fire in terms of like, you know, having an entire team for like warehouse management and, you know, inventory management and who's buying the inventory. Yeah. So, um, but then like you pointed out tech was very central to that company, it's like unique competitive advantage of like, oh, we're going to take data really seriously. And then the product itself was unique for those who aren't familiar, where it was like, you would, for as a client, you would sign up on the site and you would fill out questionnaire and like give, you know, information in certain ways. And then the company would take that information and send you a box with five items in it. And those items will be personalized to you through a combination of algorithm and then also like a human stylist. Yeah. Since then, they've kind of year to way. So it's a lot more online shopping focus now. And so that's, you know, there's pros and cons, obviously, to any business decision like that, but still like the data's like front and center there. So yeah. And in fact, and I do want to kind of zoom in on this, both companies have positioned themselves via multi-threaded the stitch, stitch, fixed blog and Netflix's technology blog itself as being at the forefront of data and tech. So what I want to start thinking about now in this conversation is how can companies think about producing sustainable business value with machine learning? And perhaps I don't know how you want to slice it. I'm interested in any way you'd like to actually, but we could look at companies where tech is at data, our first class principles, such as these companies, and then companies which are trying to develop their machine learning functions as in-comments or as smaller, you know, um, places. So how, yeah, more generally, how do you, how do you think about this? No, I think it's a really good point because that is a really key differentiator and companies. And I think it does impact the way that they view the algorithms, like, department or org or whatever you want to call it, um, and like how they how they evaluate what's going on when they add these features. So like a company like Citrix where it was very clear, like our strategy for going public, our whole strategy is competing and like what is a pretty cutthroat space, frankly, of a of and same with Netflix really, like, like, you know, entertainment, really cutthroat, uh, and clothing is really cutthroat and also very established industries. And so in both cases, um, I think that it was strategically positioned early on that like data is going to be our differentiator. I mean, Netflix had others, they really are various. Now that I'm thinking about the very similar, because Netflix started with like, we're going to send you these, like, DVDs, right? Exactly. Yeah. And it made me with mentioning that, I mean Eric Colson, who ran data at Netflix, then went to Stitch Fix. So there is even, you know, in terms of the hiring and the messaging broad and positioning of a company, there's something there. Yeah, I think, I mean, absolutely, I think that's part of why he was recruited and joined so early. He was like a very early member of the team. Um, and I think that there was some iteration with that. So it was like, at Netflix, I'm not sure what this title was there, but it's districts he like was this chief algorithm's officer. Um, and so that's like this whole new job title that like didn't really exist then. Um, and so, yeah. So like, I think in companies like that, or it's pre-IPO, and there's, this is like a newer thing, but where it's like, oh, data and algorithms are going to be our competitive advantage. So we're bringing on data people early, where prioritizing this team, um, you know, like instead of like tacking on a data scientist after a while, it's like, no, we're going to like cultivate its own vertical going up to its own like chief algorithm's officer. Um, you know, that comes with pros and cons because part of that strategy was Stitch Fix was to not have a dedicated product team for a really long time. Um, and so they built out the product team while I was there. And it was like, I don't know, like a year, less than a year before the IPO that there was finally like a VP of product and product managers and all that. So, um, so it was kind of like you could have avoided forever. And I feel strongly that these teams need to like come together, which is obviously why we're having this discussion. Um, but, you know, I think it's an evolving space. I don't think that there's perfect answers yet because like, uh, the average data scientists wanting to build machine learning products is probably a different personality type and a different coming in with different expectations than an engineer working on, you know, a software product. For example, um, and maybe that's unfair. I'm not sure that it should be, but I think realistically that's how it is right now. Um, and then, um, the two-year point, yeah, like in companies where it's not seen as a competitive advantage. You get, you see a lot of more like the tacky knowledge. So like, oh, we're going to tack on a recommender system to the bottom of this page or like we're going to um, like, I think a lot of people like are like, oh, we have this pile of data. Like, let's make, let's do data science with it. Let's make use of it. Um, and I think that that can be a pretty challenging. Each of these scenarios come with their own challenge, right? Um, and so, so yeah, I'm not sure if that actually answers your question. That's, it gets part of the way. And I do think the analogy with more classical software engineering is important both with in why it's a good analogy and why it's a bad analogy. I think where we're all trying to build software, um, and you know, from software engineering, import best developer practices. I think is something that we need to constantly be thinking about for for the database. Um, I think where a decade or two at least behind. So it makes sense that we haven't figured out division of labor, developer tools, that type of stuff in a very different state with the injection of venture capital into the tooling market, which I think is a challenge where all trying to figure out with respect to what tools we we use. Um, and what tools we we need, this is of course something we think a lot about it and out of bounds. What tools we need as as day like I personally think I should not be spending, you know, 10 hours like I did last week, mess your end with, um, Sage Maker of permissions and roles, right? So if we can abstract over that or like configuration files or what what what what what what what it is. So if we can abstract over that with tools, I think we'll get a to a close place. Um, but I do also think the demonstration of value. Um, and when it's adopted is probably a different question. So I suppose one question is when should companies and I mean, this is a very broad there will be a different quest, answer for different verticals. But when should companies adopt machine learning and when shouldn't they all what is when is it useful and when is it not that useful? Yeah, I mean, I think that so what am I big over our team philosophy is about this field right now is that as so for example, data science, you know, it's really blown up in the last 10 years and one consequence that is that there's a lot more data science training in like MBA programs or, you know, people who are like business people are coming in now with like more formal data science training, maybe not trained to be a practitioner, but trained at least to kind of understand the words that a data scientist is saying and understand how to evaluate things. And so I think that's a net positive. Um, I think that if you look back like five years, especially there was so much like hype around data science. And I think that still exists where it's like, oh, we're going to throw a bunch of money at this problem is going to magically solve it. Well, hire like a magician to come in and they're going to magically make more business value happen. Um, I just want to zoom in on that very quick. I mean, you're like, get a fight of key. There are two key concerns and then a continuum between one is people who are like, hey, we've been doing this for years, ML whatever. I don't care, right? And they stare into the void. Um, then there are the other people who say, you know, the article of Delphi or whatever it is, right? And they're like, oh, yeah, here we go. There's some like wizard wizard science that turns out to be the wizard of us in the end and then you have to, you know, get rid of a function. But where is the world in between? Like we need to get in touch with that, right? The whole continuum of possibility there as opposed to these two reasonable, but naive, an issue takes. So how do you find that space in the middle? I mean, I like I said, I think that getting more, so this is where I'm like, you know, I think traditionally data scientists have kind of bulked. And like I've said, the or in some ways that organizing principle at Citrix was like bulking at the amount of control that like product people we'd have over data science development. And like it is harder to say we're going to add this feature and this is the expected impact and like we've seen it before, so we have reasonable estimates. So there was a reason why it was like kind of magical at first of like, we don't know how much this is going to help. But I do think that having executives and product managers who are more versed and understanding how much you can reasonably expect like we're just accumulating more history like you said of this field. But I do think it's really important for product people and like the sea levels obviously, but just people involved in a client, especially user-facing product or really any product to do the same type of opportunity sizing that they might do for a more traditional feature. So it's like that idea of like is this a key competitive advantage or is this just like nice window dressing? I think is really important. I was recently, I was also, is it boxing boxing for a company as well? I mean, a lot of organizations like, hey does that involve AI and you like, I mean, I can put a logistic regression in someone. But yeah, I mean, I think that is like, it's hard not to start to sound like you're just trashing data scientists. There's like a lot of us were like it's easy, you know, like when you're looking for funding as a startup, it's like, oh, just add AI. Like you might be doing simple linear regression. But if you say AI, you're going to get more funding or he does so. And it's not just an industry. It's not just as you know, my background is in biophysics and cell biology. I mean, the amount of grant and this is not how it used to be. The amount of grants I had to just like say I'm solving cancer, while Alzheimer's in was like everyone had to have to do that, right? So yeah, we are entering the simulation of like K-FAB or a lot, a lot of this is live action role playing some of the time that that feels right? Yeah, no, you're tired. And it's like, it's not like the world you can't live in a vacuum where like everyone is being reasonable. Like, you know, VCs are dealing with a lot of information and they need to be able to like hone and quickly on what's the hot new thing and what's a true differentiator. So I get it even though it's like kind of annoying. Yeah, so where, once again, I think we're circling around this really nicely, where how do we find the value? And I love the idea of making sure that people at all levels in an organization have access to the information they need at the right time. And even, you know, I think like heads of growth, for example, don't need to understand the intricacies of, you know, multi-armed bandits, but I think I do need to understand confusion matrices and why you need to do A-A test sometimes to make sure your telemetry is correct and these sorts of things. So what type of things do people need to know for the job they need to do? Yeah, late. I think I'm confused by the actual question. So like, that was actually a statement, but it started with a what. Yeah. I was like, wait, okay. Yeah. No, I think you're totally right. Where it's like, and I think so one thing, you know, this, this thought popped into my head is you were describing that is I feel like for the business leaders, for executives, for even like product people, it can really be beneficial to have and this is gonna sound so absurd. I'm not so just me, but like having an independent advisor who can come in and do some translation for you. So like, if you're in an executive position where you really want to add machine learning, but you don't, you don't feel like you have that background to sus out. Like, can I trust the people who are telling me that they need all this money to do all this, it's gonna return like all this stuff. Having someone who can bounce ideas off who's product focused and who can say like, oh yeah, like Netflix tried this or statistics tried that or Etsy tried this, you know, and like, oh, we didn't see as a big return on this or actually there's a huge return. Like, I think, and again, maybe because I could already really get it this and I'm just singing to like, preaching to the choir, but I think that having, like not making sure that you have a advisor who you trust in order to, who aren't, don't have a vested interest in like funding the machine learning team or funding the data science team can be hugely helpful until that education gap is narrowed of like executive's learning what machine learning is exactly. For sure, that makes a lot of sense. Something that just came to mind, we were talking about Netflix and and Stitch Fix very technically sophisticated companies, right? And we do have this thing, I don't, I wouldn't put Stitch Fix in a put per se, but of, you know, a fan data science, right? And the question, whether people who want to do, let's say, what we would now call something like more reasonable scale machine learning, which perhaps is the long that the 80% or 95% we are the 99% of data practitioners, what, and we don't need Kubernetes for every, right? And I think there is a person, and we don't need GPT, like 11 for everything as well, right? So there is a question of of when building out reasonable scale, ML functions, what, what the difference is and what people need to do. So from your experience, what, what do your thoughts on that? No, it's a really good question because it's been something I've been thinking about a lot recently and really relates to your product, which is like how much do you want your ML plot? Like how much are you comfortable with it being like a black box? Because like you can develop an ML product pretty cheaply if you're just like, okay, this like out of the box thing is fine and we're going to have inputs and have something come back and we don't need to understand how it works. And like I'm on, I, so something, so okay, something I've been chewing on a lot recently is like the same way that you're generalizing, like there's so much repeated code when it comes to implementing this stuff and like you were saying for the average of scientists, it doesn't, it's not a good use of resources to have them rewriting implementation code or like redoing something that is generalizable. And so it's like, where is that boundary? Where at some point, it's like, oh, and like probably this black box recommender's this of would be like pretty good for the product that you're developing. And so I don't have any good answers. I've had that only half gets with the question that you were asking, which actually can you remind me exactly what you were asking now? It was about kind of the long-tail of data practitioners and companies and how machine learning will, will differ for them. And something I'm hearing there as well is that it's really important to be identifying business concerns and figuring out whether ML can help help to solve them, as opposed to back engineering. Hey, we have these data scientists and machine learning engineers. Let's figure out what they can do, right? And it's kind of funny because I feel like I've seen all the failure of those. Where it's like on the one hand, if you don't throw resources that try to develop a good algorithm, a good implementation of it, like a good UI for that algorithm so that the recommendations are actually compelling and not just like a tiny list at a bottom of the screen that the user never sees. Like and you can prematurely say that it's not helpful, right? And then on the other hand, I've also seen that I'm just having a different theme company. I've seen where like you pour enormous resources into like an interactive recommendation feed that's like the new home page or something. And then you don't have the opportunity to like to actually test whether or not it's going to be helpful. Like basically I've seen people throw ton of resources at what sounded like a sophisticated good recommendation system, but it didn't really map to use their behavior that well and it ended up getting retired fairly quickly. And it was like the the amount of pressure on the ops team to implement it was really high. It was really, really expensive in terms of the tooling. And so it's again, that's where I'm like, I don't think this is that different than any product development where it's like, you know, you need to opportunity size, you need to break up these these experiences into small testable chunks before you just take a huge leap. And I think that that has there are times when it really is hard like for like a social network, it's hard to like test like a small version of the social network because part of it is like how much like how many nodes do you have in the network and like you know. So it sometimes you have to take that leap of faith. But I think again, I feel like kind of the situation I was alluding to when it necessarily happened today because it's not as much of a like, oh, this is a magic wand that will save everything. So yeah, that doesn't happen so much today in like companies that have figured it out. I think in it's a great kind of warning from the future for companies that are adopting ML now. Yeah. And I think that's an important lesson. I love the idea of having small testable chunks. Yeah. Like how can you prove out empirically that something is a good idea without necessarily breaking the experience apart so much that you're testing like some small like unusable portion. And then when that test comes back as not impactful, you're like, oh, well, this won't matter. And it's like, well, you you only tested like a really handy capership of the experience. So it's not actually, you know, going to be compelling the way that the product will be. So I think that's a key file to a to a particular not being realistic about what what you're experimenting with and what the potential ROI is. Are there other file your modes that you've saying that you think could be instructive fuel? I would say the probably the so if you have a mature data team, so not not super mature, but like if you have you've like committed the resources and you're like, okay, we're going to build this and we're going forward with it. I would say the biggest trap that I see data science teams like ML product teams falling into is optimizing the wrong metrics. And so like, I mean, not, I don't know, metrics, but like optimizing the wrong outcome where it's like, I think that there can be such a divide between what the data science team wants to do. And like the I shouldn't say what they want to do, but the type of thinking where it's like, okay, give me the outcome and then I'm going to try a bunch of different fun models and do some feature engineering and I'm going to really like optimize the heck out of that outcome versus the product people like might be thinking more has holistically about the product, but aren't necessarily communicating that. And so that divide becomes very problematic, very quickly, where you might be optimizing for some sort of user behavior that if you thought about a carefully actually doesn't make it on the sense. And like this is an industry-wide thing where you see like clickbait coming from. It's like, oh, if you're going to optimize for clicks on the internet, you're going to really quickly have a race for the bottom of like inflammatory headlines and like anger and do-seen stuff. Just like playing game all the social networks right now. Yeah. Or if you optimize for view time as we know YouTube did, I mean, they're finally far more nuanced now, but it will start leading you towards more divisive um absolutely misinformation as well. And so it's like how do you as a data scientist, like it really bahoosh you to be actively involved in like defining what the user behavior will be. And I think I think a lot of people don't realize that that like the person who actually defines what you're optimizing is driving the product more than anyone else. Like, because that's ultimately like, that's like the name of the game. That's you know what you're ultimately saying people should do. And I've seen people come to the data scientists and like the data analyst including myself to be like, hey, you tell us what's important. And it's like that's actually a lot of responsibility and that can feel scary. So I can see how data scientists would be like, no, no, no, you tell me, I don't want to be a response to that. Yeah. Yeah. I actually think there's something I'll use to share a lot. It's a, this reminds me of type three errors, right, which are the the right answers, but to the wrong questions. Um, and a reminds me of a slide that Renate who is for anyone who doesn't or Renate, she's on Twitter as becoming a data scientist. She used to have a podcast that very early on about a decade ago, actually very much, she inspired me as an educator and these types of things. But she has a slide that I'll try to dig up and share at some point, which is it's about what we need to be doing is it's essentially a commutative diagram for people who know what, what that is. It's like with arrows that, you know, the loop completes whatever. Um, that was a horrible explanation of a commutative diagram. But you've got business question going to business answer. And she says we need to be factoring that through data question and data answer, but we need to make sure the mappings from business question to data question. And then data answer to business answer are what everyone thinks they actually are, right? Yeah. Oh, that's yeah, that's a really good way of thinking about it. Because, and one of the things I advocate the most, like on the podcast and sort of in my like public presence, if you will, is that data scientists are really uniquely positioned to zoom out and think about like this big picture of like, are we actually answering the right question and like how could I think, like, put on the design thinking hat or like think like a designer and say like, well, let me reframe this a little and maybe this is actually going to get it the concern. Like, this is the classic problem of someone comes to you saying, I want a number and you're like, if you're just like, due to do, here's the number done, like, that doesn't actually serve the company well and you'll probably get into like an infinite loop with them about refining that number versus if you can zoom out and like, like, when someone asks you for a number, the first question should be like, what do you need it for? Like, why are you telling me the problem and like, let's think a little bit more holistically. And I assume that's, I myself have like, we're not particularly great at asking for what we actually want, right? Yeah. That's part of our cosmic joke, I think. Well, and it's like, it's not, yeah, it's not wrong. It's a way of communicating and it's like, you know, it's a really positive interaction. If you, I think most people would respond really positively if you're like, I need this number and someone's like, okay, let me understand what you're actually saying and like, I'll learn about myself in the process and like what I actually, I'll learn about this subject area, I'll learn about like, oh, you're right. I am thinking bigger picture. Thank you for helping me, like, realize that. And so, unless it's like a really tough, I think that that dynamic of, like, give me a number, like, oh, I'm not giving you a number. Like, back in turn, talks like, so I'm sure it's not always like a happy interaction because sometimes the relationships are so strange already. Yeah. Yeah. But like, yeah, shifting that to the end, I'll product and like thinking about machine learning. It's like a very similar thing. And, and I've seen like at Stitching Swers' temple, there was a number of different outcomes. Like, we send someone to item and fix and then they buy it and they still have a survey about what they like or not. And it was pretty fruitful to start playing around with what that outcome was to say, like, okay, it's not just that you buy it. It's that you buy it and you don't complain or you buy it and you said you love the style or like you said you loved it, but you sent it back because it was the wrong size. Like, there's a lot of nuance there. And again, you have to understand that you have to behave here and this the person actually, like, it's like, let's let's the bias and that data said it's the people who are even willing to sit down and fill out like, you know, the survey at the end of the fix. And so there's a lot there, but I think it's good for the data scientists know that and it's good for the product people to know that so that they can work together in a really healthy way to come up with like the right algorithm algorithmic experiences. For sure. So I want to jump into that now. I do want to just make two quick statements before that. This challenge of connecting the data function to the decision function is such a big challenge that, you know, even a couple of years ago, McKinsey, I don't know whether it's taken on in or honestly McKinsey, like, was pushing and created a local data translate, which is essentially someone to be that interface to making sure communication shows. I didn't know that. That's super interesting. Yeah. Yeah. And the other thing, we'll discuss in proxies and I direct everyone to Rachel Thomas has a beautiful post on proxies in data science and machine learning and what's known as good heart store, right? If you're optimizing for something, you actually end up doing things to hack it. And there's, I think there's the apocryphal tale of a nail factory where people are told to make nails that collectively weigh the most. Like they're optimizing for total total mass. So they just create really large, like one really giant, really heavy nail. And that's, I know we want to optimize for the total number of nails. So then they create a lot of really bad, really tiny ones, right? Yeah. So I've heard that, yes, I've heard that anecdote, but it was in connection to like the USSR. Like, I wonder if that's just that tale has more stories like, it's like growing up in America. It's like, this is wrong with communists. Yeah. Yeah. No, it definitely sounds like Western hegemonic propaganda to me. So what I'm interested in is I've got a kind of a constellation of questions that we can just riff on in any anyway. How does product management meet data science and machine learning? And how can people think how can data scientists think about this? How can product managers think about this? And particularly one thing, I mean, we know data teams, we've got different like we've got embedded structures and like centers of excellence and hybrid structures with product management, like I hate this term, but product managers generally have a lot of soft power. Like people say, you're the CEO of a product. That's clearly not true, right? In terms of, I think the leverage a CEO versus a PM has to be honest. But when we have such kind of roles that are in flux, how do even these two meet? And disciplines which are evolving very quickly. One of the thing I'll throw into the mix is because of data science products don't exist. And what I mean by that is there's not like what is Google search? I mean, there is no static product. There are so many experiments happening happening constantly that it's kind of, you know, you can never even step in the same river once or something like that, right? Because it's moving so quickly. Yeah. So that was a bunch. That was a brain dump, right? But well, no, it's yeah. And I've spent so much time thinking about this. And I don't know that I have the answers, but I can certainly say what I've been thinking about, which is that I totally like, and this is where again, the education gap comes in, where it's like, is there a future where product managers come in with such sophisticated understanding of algorithms that it's like, they can just pee on the whole thing. But we're definitely not there right now. So it's like, if you have, you know, a product that has a lot of algorithms, it's like a, I keep saying recommender system, so I'm just going to keep saying that, but it's like, if you have a recommender system, and it is also part of a user facing product that is not like super simplistic. So I mean, even like the Amazon, you know, recommended thing. There's a lot of power decisions that go. And this also, it generalizes to search and fades and all of these things so it is a very general term in that sense. Yeah. And so it's like, at the end of the day, I have not yet been convinced that one person is capable of p.m.ing the entire thing. So it's like the user research and the algorithm development and the product development and the engineering, you know, like, like the paths that I've worked with are usually doing the user research, they're working with the designers and they're doing the engineering stuff. But then the algorithm itself is like tossed over to the algorithm team. And so then it's falling into the hands of like a single person or some sort of central al-mal team that itself has to interface with this product person, right? And so this is exactly what you're talking about where it's like, should you have a translation layer between that and the p.m. because like the p.m. is the translation layer between like the user researchers and the engineers. I mean, usually they're all working together and it's not like a wall. But that the p.m. is facilitating that relationship and making sure that the insights from one are getting to the other and like doing design sprints and, you know, there's lots of different layers that that product development happens in a mature way. And you know, as we've kind of been circling around, like I just haven't seen that as much in the data world. And I'm saying that as much because I'm like, I feel like I can just hear people being like, we do that, you know, it's certainly like I feel like whenever I bring this up with other data scientists who've been around the block, they're like, yes, this is a problem. Like it doesn't happen the way you want it to. And so, and I think what exacerbates this problem is that you, unless you're like a really big company, I haven't, and I think there's some resistance to this of like having an algorithms person who's like a tech lead versus the algorithms person who's like this interface layer. And so it kind of will end up falling on the manager shoulder to do both. And at least in the companies I'm aware of and like what I've seen usually the managers of the algorithms team are promoted because they're like really strong ICs. They know a ton of machine learning like the data scientists want to learn machine learning from that person. And that's great for that leg of the development. But in terms of like interfacing with the product people that does the noise like that's not necessarily the optimal person for that. And like I said, I've just, I think that the machine learning teams that I've been familiar with have this allergic reaction to like these translation layers because it's like like it's usually it'll be people from academia or it's people who like are just like opinionated or I don't know why but it's people who are kind of like, hey, I can manage this myself. Like it's insulting that you think that I can't like interface with this person. Like it gets, I've seen it just breakdown a lot basically. Yeah. And I understand that I think, um, whenever you introduce, I'm highly skeptical of translation layers, whenever you introduce another human and another role into the mix, it introduces a whole bunch of complexity that we need, we need to figure out whether those externalities are worth it, essentially, right? Yeah. But then it's also like, how are you realistic about like, let's just too much work for one person? And like what is someone happy doing? Like I think that there's been a move within data science to like, oh, you need to be full stack and you need to understand like all parts of the business. And it's like, but wouldn't it's like similar to like the tooling your building. It's like wouldn't data scientists be happier not doing this part of the job like because it's not necessarily adding value. And so it's like, it's, I mean, that's kind of like the whole point of a p.m. is to like remove roadblocks and like make your life easier and get projects moving smoothly so that you can focus on what you want to. And I'm not convinced that every person building a machine learning engine of some sort, like, wants to be doing that level of product development. Because like personally, I think that there should be a data scientist in the room and like doing design. Like I think they should be in the design sprint saying like, oh, okay, well, the way you're the way the designers imagining this recommender system, like the way you have it now is actually pretty hard to do, pretty hard to predict, but we could like change it a little this way and it would be easier. And like maybe we need to collect new data in order for this thing to work while. So like, let's also develop this new data collection layer in order to do this thing better. So like that's again, that's not necessarily something that someone who wants to be writing code all day is going to want to do. So yeah, I don't like, like, like, I don't think I have the answer, like I said, even though I like to think that I know everything. And well, if you had the answer, I wouldn't invite you on a public live stream to do it. I'd suggest me to build a company together and try to get get some of that upside. I don't think we've figured it out as a discipline and a community, we're still figuring out really significant division of labor questions as well. And as you alluded to, that is something we, I mean, the paradox of the, I'm, uh, Ville and I are teaching a sort of Ville works on metaphor and it's the CEO of Adabounds. We're teaching a tutorial together at ODSC. Cool. And yeah, it's great. And so it's, out of tutorials called full stack machine learning for data scientists. And I had to like, I wrote a thread on Twitter essentially that explained why I don't like that terminal and why we needed to use it. Because I don't want to, um, create any full stack machine learners or data scientists. But what I think we need to do is have tools which allow them to focus on the parts of the stack that they want to focus on, and give them access to the, to the rest and edgica. So you need to know about compute layers and that type of stuff, whether you need to know how to like configure all of, you know, than the Kubernetes stuff, um, yes, is, is another question. Um, you know, another thing that I think again, because software engineering is just a little more baked than data science is like a more realistic discussion of like, I totally see why full stack data scientists was a thing. It's, it is realistically like it is a thing for a small startup where it's like, you're going to need one person to come and ask. Yeah, sure. Yeah. And so it's like, it makes sense that like when the field was new and like it wasn't you just established teams. It was one person. It was like, yeah, we need someone who can like get the data and can do the models and can like write the code to get it into the product and like, yeah. But the joke is of course, they're not a data scientist that all the data at least they'd start up. They're a data engineer for 12 to 18. Right. Yeah, exactly. And so it's like, it, so I think that there's like the continuum of stable versus nimble, right? And I think that in the engineering world, you're more familiar with like, okay, we're like, we need to be more stable versus less nimble. But I think that that again, like, I can feel people being like, I do that. But like, I feel like the average data scientist isn't probably as experienced at making those tradeoffs and like being like, yes, my job is not going to be more annoying in order to make this more stable. Like, I think there's a huge drive like, oh, I want to keep it nimbles. I can be creative and I can adapt and I can, you know, like, create more, you know, and I'm worried about we're not creating a business value with this algorithm. So I need to keep that, like, like nimbleness. So yeah, it's like, I don't, it's funny because it's conversation. I'm like, this is really hard. But it is. There's a lot, it's like, there's, again, there's complex tradeoffs because it's not like, there's a lot of ways to make a data feature better, like, collecting better data or changing the outcome or, you know, building a bottle that takes into account by a better so that when I check out from Amazon, I bought a trashcan. It doesn't tell me to buy, like, 22 more trashcans. You know, it's like, there are ways to start to adapt an algorithm to deal with that versus, like, an engineering feature, like, aside from, and I don't want to sound like I'm down plain, but usually it's like, okay, where's the latency? Is this doing what we thought? Like, we could design it differently, but it's not like the gut's are going to change that much. So who does product machine learning product management really well in your mind? What can we learn from them? I don't, I honestly don't know the answer to that question. That's great. Yeah. I'm hearing there's a lot of, nobody doesn't necessarily listen to Superwell. I don't know the inner workings of every tech company enough to be like, I don't know. I could in point to one company and be like, they're nailing it. One time I had a conversation with someone at a, at like Thanksgiving, I was like, like, a friend giving and it was another person there who, and I was like, that guy really got it, like, where he had built out, and it wasn't even a machine learning team, but he had built out. It was, he was working in customer service for like one of these big, like financial companies where you're like, I don't, I don't even know how to describe it. It was like an investing, like personal investing type thing. And so they have clients who are people, and they have to, they have a big customer service team who's deciding like, are these people like going to turn, should we do more personalized outreach? Like, how do I keep my clients? So client retention basically. And so he had taken a few of the people who were interested in building models and created its own like data science team within this org in order to create data-driven scores to deliver to these teams in order to say like, okay, in addition, like, for the customer service people who are evaluated on how many clients they retain, we've now produced this number that you can use if you choose to in order to manage like the client relationships. And then presumably, the people using it are like, oh, this is a helpful thing. So like, I'm going to use it more. But like, the tenants that I thought were really, like, the reason I was like, he did this really well, is because taking practitioners and making them the people who were building the models, I think is really key for trust. And then I think that making the product is self-optional for the consumers. So it's like they still had the autonomy of their job. Like, it seemed like a really well-defined output, like, product for the CS representatives. Because it was like, oh, now you can, like, you're not going to blame the algorithm and like shake your fist at it, like, because you're choosing to use it or not. So you have, you still have your autonomy. But that's obviously not the way to build like the world's most sophisticated model. And so it's, but do we even need the world's most sophisticated model, like most of the time? I feel like no, which is why I'm like, how often would blackbox work, you know? And is it, is like, so one way to, for example, like make this like in between-brill work is by saying, like, okay, we're actually going to like contract the machine learning out to someone else, right? And then the person who's that contact for the contract is like the person who's that translation layer. And that's where I'm like, I don't, I couldn't tell you when like, that makes sense versus ramping up, like someone who doesn't have the background who wants to learn this, like, very, like, highly valuable skill, like, went to that make more sense versus went doesn't make sense to build this internal team. Look, this is such a key point where my mind and many conversations I have around these things end up going this direction. But essentially one thing we need to do when we're trying to do here is pattern recognition, right? And figure out what, what's appropriate. And the thing is, we are at such an early stage that pattern, where we're just getting an idea of the landscape and finding out compass essentially, right? So we're trying to kind of form these patterns to see what we, what we can and can't do and what's appropriate. Yeah, plus things. And having something that is adaptable. So it's like maybe you start with like a black box solution using like generalize tooling. And then you also have your roadmap for like becoming more sophisticated on whatever, you know, so it's like, okay, eventually we want to control the guts of this, but we still want the tooling to be generalized or like, you know, eventually you get to the place for like you've built your own custom tooling. Although I'm also like bad. I don't know that that like actually makes sense. But there's path dependencies, though, where it's like, I think that what I've seen at these companies that I've been at and like other companies I know is that if you like, invest really heavily at one modeling framework or like you just you build like brittleness into your models, then you can't like do the hot new thing. And there is like a major disadvantage to that. So so basically it's hard. Product level machine learning is really hard. So I'm really we've got around five minutes left. And I'd love to think about two things. The first is then they're correlated. The first is, um, who should be thinking about doing paming ML stuff? Should it be pms learning wanting to learn their ML side or data practitioners wanting to learn more about product management? Um, and I think we have more data practitioners here than people who are pms. So after that maybe we can let them know things that they could learn to help them and learn more product management. No, I think I mean, A, I think any product manager would be very well served to learn data science stuff. I should say. So like, you know, getting a more sophisticated understanding of how these algorithms work or how like optimization works. Like, I think that's that's just a good idea. Um, on the data practitioner side, like, I think that like, my first impulse is to say people who are interested in management. I think this is really critical. Um, and again, I think that the field pushes toward people with like fancy technical skills to become the managers. But I think realistically if you want your team and especially the company to succeed, I think that like getting understanding like the product management side and how to like both retain the trust of your team, but also push them toward like meeting the product needs. Like, if you're not having the team directly interface with product all the time, then being that translation, it makes a lot of sense for the manager to be that translation layer. Like, there already do lean that with like HR, like, you know, there, like, that is who they are for like every other part of the company. Um, and then, uh, and then yeah, I think that as I've seen so many people interested in this data PM rule. And so I do think I think one thing that is super critical for now is that in order for data people to actually trust the PM to tell them what to do, like being pretty strong on the data front is key. And so I think that having someone who is a data scientist who's like doing more product management, um, I think that's probably the only path in a lot of companies, um, for how to, so you have to have a company that supports this idea generally, which I think I've miscalculated in the past, frankly. But I think that there is the potential for like really fruitful relationships there, assuming that like everyone's kind of on board with the idea. So kind of what people just still push that forward, even though I've seen it fail when like data people are like, you don't need to get in between me and the product team. I'm fine or like, you know, manager, like there's lots of ways where taking on project management, I think, can rub data people the wrong way. So it's not perfect, but I still think people should be trying to do it, basically. Yeah, absolutely. I appreciate that. And something we haven't even talked about that maybe we can do another time is, I mean, product management's such a fascinating field in that it's and discipline in that it's not, as well, like you don't go to school for product management, right? Like you enter companies usually in a different type of role and take it on and and and and these types of things. And I think it is probably probably emerged originally like as project management in Microsoft or something like that when when this type of role started to and there are a couple of books and that type of stuff, but you really learn about it on the job. As a, I just thought of a slightly provocative question. That it might be fun to wrap up on. Where's the provocation though? I think my, my real question is will all software soon be powered by machine learning and will all PMs be machine learning PMs and I just want to clarify by machine learning. I don't mean machine learning actually. I mean powered by data of some, like introducing the entropy and complexity of the real world through an interface with real world data in in in in some sense. Yeah, that's a good question that I've really never thought of like I could see like you're saying kind of like adaptive design. So immediately I was like trying to think of simple products where I'm like, would that really need it? But then my head was like, well, honestly, like if a product were like seeing okay, she uses these features in this way. So that's a depth of the product to like highlight those features more so that it's easier for her to use like that actually seems like pretty good idea for almost all products. So, so yeah, I don't know. That's a really good question. I do think like the more simple version of that question of like will understand a machine learning be critical for PMs like going forward. I think that answer is yes and also for executives where it's like you just again like you're kind of vulnerable to the data science team telling you whatever you know like pulling the wool over your eyes if you don't have the ability to push back and does like have that conversation with like a level of knowledge. Like I think that's a big thing for managing. You have to make sure that the people your managing are not like taking advantage of you know. Yeah. And just like telling you, oh, that's really hard and you don't understand it and you're like, okay, I guess that's truly and so I do think that's critical. I don't think, I mean, even just with experimentation, I think it's really important for PMs to be able to let that check. Yeah. And actually, I'm glad you mentioned experimentation because I want to tie this back to product development and tool building more generally. I think the introduction of tools such as optimisedly is one of my favorite examples of how that changed the game for what everyone in an old needs needs to know based on the value that technology can deliver. So the fact that non technical people could suddenly start doing online experiments with their their websites and that type of stuff was an absolute game changer and similarly something like um, I'll give the example of the totally unproductive. Sorry, an unprovocative example of using machine learning and hiring flows for resume screening. Right. Like you don't necessarily need to know about gradient descent if you're using that, but you better know about confusion matrices and a better I saw recently like from from TFX import fairness and a better not be an API that does something like that, right? So you need to be deeply deeply aware of these things. And maybe there's a broader uh, I mean part of the reason we I think there's an incentive to do use machine learning my months just firing in a couple of different directions right there. So I'm trying to try to write it in. If you're one of the reasons you it may be useful or you have an incentive to do it is the amount of resumes you get the amount of information you're and I think we are we have hit this some inflection point from information scarcity uh landscape to information abundance, right? And I always use the analogy of when you know we started having food abundance in in the West suddenly for example you you need to like previously if you see food you eat it because food scarce and you need it right? Yeah. Suddenly when there's a huge abundance of food you need to have a different relation otherwise you get like nation wide obesity and such it says we have a big problem with in in Australia for example and in in America of of course so what happens? What changes when we move from information scarcity to information abundance and the importance of machine learning there I think is key purely in terms of ranking essential right? Yeah. Yeah. What's important to take in and what's important to not pay attention to at the moment and resumes coming back to resumes that's what when you get a lot of resumes coming in because of the technology that allows that to happen you you need that and what to people need to know and these are the things that we're getting out now that is I think with my progress it's still there's still a lot of topology to explore I think. I like that and now that's not really good analogy I really like that where it's like yeah we're in I mean yeah the information age man there's new things complicated there's a really good documentary and book about oh my gosh I'm not gonna remember it the guy who like essentially invented information theory like mathematician. Port Shannon. Yes and have you read like it's like a mind it play and then there's also like it's like a doc you've been hearing but it's like acted out with people but it's basically an interview yeah I saw that there was like an event in San Francisco where they did that. I love some ray and actments in documentaries. Yeah it well it was a little confusing because I didn't realize it I was like God these conversations are like so Hollywood I was like outright because they are actually not that's not actually him but I think he would have been very very ambivalent about like the result of his research like the information age that he essentially unlocked because he was not he was like you know ultimately went to like designing unicycles and like juggling and that's like he didn't like engage in the like information gathering behavior after that so anyway yeah information age is like I have some real I have ups and downs with that like you have to chat about that another time and in fact I've actually got this book next to me I'm using it as a microphone stand James Blackwood but this is a beautiful on my screen isn't it's called the information it's an incredible book about information theory it starts off with um uh form of civilizations that would communicate across great distances by playing drums and the rhythm of their drums would communicate through to Napoleon's control of mechanical telegraphs through to the information age beautiful descriptions of entropy in there as well. James Black has been on my mind recently because he's he's been an advocate of removing time zones from the world completely which I think is you know relevant now with um daylight savings forever um I feel like we could talk about this for so long we do need to wrap up we've got some really interesting questions and I'm gonna ask people to put in the put in the slack channel because we do need to wrap up but for example there's one about the pm topic being a chicken and egg sort of problem orgs needs pms that can code um but it's tough could convince those who can move into such roles and if they do they don't have the pm skills so how to figure out these these types of things incredible questions that um I'll get everyone to ask in slack and that's to remind everyone we're having um and ask me anything which will it'll be icing so um if you join our community slack I'm gonna put the link in the chat again um I'm just over the next week or so ask questions early but Hillary will will be popping in every now and then over the next week or so if you have any questions for me um I may not be as lucid or clear as Hillary but feel free as as well um I'd like to thank everyone for joining um if you enjoyed this please do share it with friends and hit hit subscribe because we're just getting getting started as well so thank you all for joining but of course thank you so much Hillary this has been such a fun conversation and it's always great to chat with you but to do this for the first time publicly has been in a real trait yeah yeah no thank you so I just was great and um yeah I thank you to everyone who listen even though I cannot see you I appreciate and you'll see you'll see them on slack um so we'll we'll we'll see you on on on on on the slack everyone and thanks once again and see you!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "url = 'https://www.youtube.com/watch?v=Dr6DsWa6Dhg'\n",
    "model_type = 'tiny'\n",
    "transcription_task = make_task(url, model_type)\n",
    "fs_chat_transcription = transcribe_video(transcription_task)\n",
    "fs_chat_transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22bd25-643c-4d84-adbe-ca638db1424d",
   "metadata": {},
   "source": [
    "# Running Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f162e-e53e-474b-858f-69a70a079578",
   "metadata": {},
   "source": [
    "## Transcribe one Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eb6605e-d14f-4766-af7f-eecbdfa93629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.7.11\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mYouTubeVideoTranscription\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:eddie\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m\u001b[22mRunning pylint...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    Pylint is happy!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[35m2022-10-08 18:56:08.964 \u001b[0m\u001b[1mWorkflow starting (run-id 184193):\u001b[0m\n",
      "\u001b[35m2022-10-08 18:56:10.591 \u001b[0m\u001b[32m[184193/start/1001752 (pid 64624)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:56:18.503 \u001b[0m\u001b[32m[184193/start/1001752 (pid 64624)] \u001b[0m\u001b[1mForeach yields 1 child steps.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:56:18.503 \u001b[0m\u001b[32m[184193/start/1001752 (pid 64624)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:56:21.215 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:56:23.142 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Task is starting (Pod is pending, Container is waiting - ContainerCreating)...\u001b[0m\n",
      "\u001b[35m2022-10-08 18:56:53.238 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Task is starting (Pod is pending, Container is waiting - ContainerCreating)...\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:23.913 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Task is starting (Pod is pending, Container is waiting - ContainerCreating)...\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:06.821 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Setting up task environment.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:23.286 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Downloading code package...\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:24.048 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Code package downloaded.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:24.070 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Task is starting.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:25.413 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b]\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:25.413 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Installing Python packages using pip...\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:25.730 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: jiwer in /usr/local/lib/python3.10/site-packages (2.5.1)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:25.733 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: levenshtein==0.20.2 in /usr/local/lib/python3.10/site-packages (from jiwer) (0.20.2)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:25.736 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.10/site-packages (from levenshtein==0.20.2->jiwer) (2.11.1)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:26.565 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Collecting git+https://github.com/openai/whisper.git\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:26.566 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b]   Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-w_fp_ld3\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:27.666 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b]   Resolved https://github.com/openai/whisper.git to commit 9e653bd0ea0f1e9493cb4939733e9de249493cfb\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:27.668 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b]   Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:26.171 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:26.567 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b]   Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-w_fp_ld3\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:27.874 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b]   Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:27.880 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from whisper==1.0) (1.23.3)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:27.880 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (from whisper==1.0) (1.12.1)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:27.881 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from whisper==1.0) (4.64.1)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:27.881 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/site-packages (from whisper==1.0) (8.14.0)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:27.882 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: transformers>=4.19.0 in /usr/local/lib/python3.10/site-packages (from whisper==1.0) (4.22.2)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:27.883 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.10/site-packages (from whisper==1.0) (0.2.0)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:27.888 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: future in /usr/local/lib/python3.10/site-packages (from ffmpeg-python==0.2.0->whisper==1.0) (0.18.2)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.185 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers>=4.19.0->whisper==1.0) (21.3)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.185 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers>=4.19.0->whisper==1.0) (2.28.1)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.186 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.10/site-packages (from transformers>=4.19.0->whisper==1.0) (0.10.0)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.187 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers>=4.19.0->whisper==1.0) (5.4.1)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.187 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/site-packages (from transformers>=4.19.0->whisper==1.0) (0.12.1)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.188 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers>=4.19.0->whisper==1.0) (3.8.0)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.188 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers>=4.19.0->whisper==1.0) (2022.9.13)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.199 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch->whisper==1.0) (4.4.0)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.233 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.19.0->whisper==1.0) (3.0.9)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.255 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers>=4.19.0->whisper==1.0) (2022.9.24)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.256 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers>=4.19.0->whisper==1.0) (1.26.12)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.257 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers>=4.19.0->whisper==1.0) (2.1.1)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.258 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers>=4.19.0->whisper==1.0) (3.4)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.729 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: pytube==12.1.0 in /usr/local/lib/python3.10/site-packages (12.1.0)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:28.352 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:29.154 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:29.554 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.10/site-packages (0.2.0)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:29.560 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Requirement already satisfied: future in /usr/local/lib/python3.10/site-packages (from ffmpeg-python==0.2.0) (0.18.2)\u001b[0m\n",
      "\u001b[35m2022-10-08 18:57:29.975 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "100%|███████████████████████████████████████| 461M/461M [00:10<00:00, 47.0MiB/s]\u001b[0m22m[pod t-vll8s-k8n7b] \n",
      "\u001b[35m2022-10-08 18:58:45.042 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[22m[pod t-vll8s-k8n7b] Task finished with exit code 0.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:58:46.079 \u001b[0m\u001b[32m[184193/transcribe/1001753 (pid 64634)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:58:47.573 \u001b[0m\u001b[32m[184193/postprocess_transcription/1001754 (pid 64652)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:58:54.863 \u001b[0m\u001b[32m[184193/postprocess_transcription/1001754 (pid 64652)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:58:56.142 \u001b[0m\u001b[32m[184193/end/1001755 (pid 64656)] \u001b[0m\u001b[1mTask is starting.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:59:00.787 \u001b[0m\u001b[32m[184193/end/1001755 (pid 64656)] \u001b[0m\u001b[1mTask finished successfully.\u001b[0m\n",
      "\u001b[35m2022-10-08 18:59:01.093 \u001b[0m\u001b[1mDone!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! python youtube_video_transcriber.py run --url 'https://www.youtube.com/watch?v=ZEcqHA7dbwM'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ceda2-61da-4942-a1bc-3eae82258338",
   "metadata": {},
   "source": [
    "## Transcribe each Video in a Playlist\n",
    "\n",
    "[This url](https://www.youtube.com/playlist?list=PLUsOvkBBnJBc1fcDQEOPJ77pMcE4CnNxc) goes to the playlist for Ville's [tagging blog](/blog/five-ways-to-use-the-new-metaflow-tags/). The playlist consists of 5 videos:\n",
    "* [Basic Tagging](https://www.youtube.com/watch?v=DEmKaTI3MG4&list=PLUsOvkBBnJBc1fcDQEOPJ77pMcE4CnNxc&index=1): 05:41\n",
    "* [Programmatic Tagging](https://www.youtube.com/watch?v=25Hqp43J37I&list=PLUsOvkBBnJBc1fcDQEOPJ77pMcE4CnNxc&index=2): 04:52\n",
    "* [Tags and Namespaces](https://www.youtube.com/watch?v=ifARsmiSNhE&list=PLUsOvkBBnJBc1fcDQEOPJ77pMcE4CnNxc&index=3): 10:34\n",
    "* [Tags in CI/CD](https://www.youtube.com/watch?v=hIiDXPHqEFM&list=PLUsOvkBBnJBc1fcDQEOPJ77pMcE4CnNxc&index=4): 03:28\n",
    "* [Tags and Continuous Training](https://www.youtube.com/watch?v=lZhwhuG0AN8&list=PLUsOvkBBnJBc1fcDQEOPJ77pMcE4CnNxc&index=5): 04:33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec80592d-1606-48ce-b19a-a3c5294ee8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/eddie/Dev/operationalizing-whisper/youtube_video_transcriber.py\", line 11, in <module>\n",
      "    class YouTubeVideoTranscription(FlowSpec):\n",
      "  File \"/Users/eddie/Dev/operationalizing-whisper/youtube_video_transcriber.py\", line 51, in YouTubeVideoTranscription\n",
      "    flag = int(os.getenv('IS_REMOTE'))\n",
      "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n"
     ]
    }
   ],
   "source": [
    "! python youtube_video_transcriber.py run \\\n",
    "    --url 'https://www.youtube.com/playlist?list=PLUsOvkBBnJBc1fcDQEOPJ77pMcE4CnNxc' \\\n",
    "    --m 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f946db52-0d4b-42ce-b275-a6019453784f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "      <th>model_type</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>views</th>\n",
       "      <th>length (s)</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>publish date</th>\n",
       "      <th>transcribed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://youtube.com/watch?v=hIiDXPHqEFM</td>\n",
       "      <td>metaflow_tags_tags_in_cicd</td>\n",
       "      <td>small</td>\n",
       "      <td>Metaflow Tags: Tags in CI/CD</td>\n",
       "      <td>Outerbounds</td>\n",
       "      <td>45</td>\n",
       "      <td>208</td>\n",
       "      <td>Learn how to use Metaflow tags in conjunction ...</td>\n",
       "      <td>hIiDXPHqEFM</td>\n",
       "      <td>2022-06-21</td>\n",
       "      <td>Let's turn into more production-oriented use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://youtube.com/watch?v=lZhwhuG0AN8</td>\n",
       "      <td>metaflow_tags_tags_and_continuous_training</td>\n",
       "      <td>small</td>\n",
       "      <td>Metaflow Tags: Tags and Continuous Training</td>\n",
       "      <td>Outerbounds</td>\n",
       "      <td>27</td>\n",
       "      <td>272</td>\n",
       "      <td>Learn how to use tags to facilitate continuous...</td>\n",
       "      <td>lZhwhuG0AN8</td>\n",
       "      <td>2022-06-21</td>\n",
       "      <td>This last example might be one of the most si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://youtube.com/watch?v=25Hqp43J37I</td>\n",
       "      <td>metaflow_tags_programmatic_tagging</td>\n",
       "      <td>small</td>\n",
       "      <td>Metaflow Tags: Programmatic Tagging</td>\n",
       "      <td>Outerbounds</td>\n",
       "      <td>31</td>\n",
       "      <td>291</td>\n",
       "      <td>Learn how to use Metaflow tags programmaticall...</td>\n",
       "      <td>25Hqp43J37I</td>\n",
       "      <td>2022-06-21</td>\n",
       "      <td>In some sense this example is an inverse of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://youtube.com/watch?v=ifARsmiSNhE</td>\n",
       "      <td>metaflow_tags_tags_and_namespaces</td>\n",
       "      <td>small</td>\n",
       "      <td>Metaflow Tags: Tags and Namespaces</td>\n",
       "      <td>Outerbounds</td>\n",
       "      <td>17</td>\n",
       "      <td>633</td>\n",
       "      <td>Learn how to use Metaflow tags to create isola...</td>\n",
       "      <td>ifARsmiSNhE</td>\n",
       "      <td>2022-06-21</td>\n",
       "      <td>From the last two examples, you may have got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://youtube.com/watch?v=DEmKaTI3MG4</td>\n",
       "      <td>metaflow_tags_basic_tagging</td>\n",
       "      <td>small</td>\n",
       "      <td>Metaflow Tags: Basic Tagging</td>\n",
       "      <td>Outerbounds</td>\n",
       "      <td>69</td>\n",
       "      <td>341</td>\n",
       "      <td>Learn how to use Metaflow tags for experiment ...</td>\n",
       "      <td>DEmKaTI3MG4</td>\n",
       "      <td>2022-06-21</td>\n",
       "      <td>Let's start with the very simple use case for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       url  \\\n",
       "0  https://youtube.com/watch?v=hIiDXPHqEFM   \n",
       "1  https://youtube.com/watch?v=lZhwhuG0AN8   \n",
       "2  https://youtube.com/watch?v=25Hqp43J37I   \n",
       "3  https://youtube.com/watch?v=ifARsmiSNhE   \n",
       "4  https://youtube.com/watch?v=DEmKaTI3MG4   \n",
       "\n",
       "                                     filename model_type  \\\n",
       "0                  metaflow_tags_tags_in_cicd      small   \n",
       "1  metaflow_tags_tags_and_continuous_training      small   \n",
       "2          metaflow_tags_programmatic_tagging      small   \n",
       "3           metaflow_tags_tags_and_namespaces      small   \n",
       "4                 metaflow_tags_basic_tagging      small   \n",
       "\n",
       "                                         title       author  views  \\\n",
       "0                 Metaflow Tags: Tags in CI/CD  Outerbounds     45   \n",
       "1  Metaflow Tags: Tags and Continuous Training  Outerbounds     27   \n",
       "2          Metaflow Tags: Programmatic Tagging  Outerbounds     31   \n",
       "3           Metaflow Tags: Tags and Namespaces  Outerbounds     17   \n",
       "4                 Metaflow Tags: Basic Tagging  Outerbounds     69   \n",
       "\n",
       "   length (s)                                        description           id  \\\n",
       "0         208  Learn how to use Metaflow tags in conjunction ...  hIiDXPHqEFM   \n",
       "1         272  Learn how to use tags to facilitate continuous...  lZhwhuG0AN8   \n",
       "2         291  Learn how to use Metaflow tags programmaticall...  25Hqp43J37I   \n",
       "3         633  Learn how to use Metaflow tags to create isola...  ifARsmiSNhE   \n",
       "4         341  Learn how to use Metaflow tags for experiment ...  DEmKaTI3MG4   \n",
       "\n",
       "  publish date                                   transcribed_text  \n",
       "0   2022-06-21   Let's turn into more production-oriented use ...  \n",
       "1   2022-06-21   This last example might be one of the most si...  \n",
       "2   2022-06-21   In some sense this example is an inverse of t...  \n",
       "3   2022-06-21   From the last two examples, you may have got ...  \n",
       "4   2022-06-21   Let's start with the very simple use case for...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = Flow('VideoTranscription').latest_successful_run\n",
    "run.data.results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c48f5d4-ff73-48cc-94f1-5f2dc75ec42b",
   "metadata": {},
   "source": [
    "## Transcribe a List of Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344c36b-599c-4bb6-9053-332800391286",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python youtube_video_transcriber.py run --urls 'science_video_urls.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaab3c3-a302-4a31-be63-4f2d4168e9c0",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02265c1c-b405-414f-8818-81f43fb1cef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17 minutes ago'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import humanize\n",
    "import datetime as dt\n",
    "from metaflow import Flow\n",
    "run = Flow('YouTubeVideoTranscription').latest_successful_run\n",
    "humanize.naturaltime(dt.datetime.now() - run.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04341065-66a5-424f-8599-f4b034a9d4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "      <th>model_type</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>views</th>\n",
       "      <th>length</th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>transcription_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://youtube.com/watch?v=GVsUOuSjvcg</td>\n",
       "      <td>future_computers_will_be_radically_different_a...</td>\n",
       "      <td>tiny</td>\n",
       "      <td>Future Computers Will Be Radically Different (...</td>\n",
       "      <td>Veritasium</td>\n",
       "      <td>8672686</td>\n",
       "      <td>1302</td>\n",
       "      <td>Visit https://brilliant.org/Veritasium/ to get...</td>\n",
       "      <td>GVsUOuSjvcg</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>For hundreds of years, analog computers were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://youtube.com/watch?v=ieDIpgso4no</td>\n",
       "      <td>breaking_new_phase_of_matter</td>\n",
       "      <td>tiny</td>\n",
       "      <td>BREAKING: New Phase of Matter</td>\n",
       "      <td>Physics Girl</td>\n",
       "      <td>1445568</td>\n",
       "      <td>934</td>\n",
       "      <td>What are time crystals? How do scientists make...</td>\n",
       "      <td>ieDIpgso4no</td>\n",
       "      <td>2022-06-15</td>\n",
       "      <td>We're filming a video about time crystals. Ok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://youtube.com/watch?v=Ek1buV2HA68</td>\n",
       "      <td>worlds_only_moving_mud_puddle</td>\n",
       "      <td>tiny</td>\n",
       "      <td>World's Only Moving Mud Puddle</td>\n",
       "      <td>Physics Girl</td>\n",
       "      <td>4100457</td>\n",
       "      <td>699</td>\n",
       "      <td>Want to support more videos like this? Patreon...</td>\n",
       "      <td>Ek1buV2HA68</td>\n",
       "      <td>2021-05-10</td>\n",
       "      <td>Hey, let's pull them straight in line in here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://youtube.com/watch?v=ccO0_hSXMyM</td>\n",
       "      <td>how_mirrors_could_solve_our_energy_problem</td>\n",
       "      <td>tiny</td>\n",
       "      <td>How Mirrors Could Solve our Energy Problem</td>\n",
       "      <td>Physics Girl</td>\n",
       "      <td>1072714</td>\n",
       "      <td>690</td>\n",
       "      <td>We visited a giant field of solar mirrors to l...</td>\n",
       "      <td>ccO0_hSXMyM</td>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>This is 4,000 acres of mirrors. As you drive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://youtube.com/watch?v=S2xHZPH5Sng</td>\n",
       "      <td>clickbait_is_unreasonably_effective</td>\n",
       "      <td>tiny</td>\n",
       "      <td>Clickbait is Unreasonably Effective</td>\n",
       "      <td>Veritasium</td>\n",
       "      <td>6098423</td>\n",
       "      <td>1165</td>\n",
       "      <td>The title and thumbnail play a huge role in a ...</td>\n",
       "      <td>S2xHZPH5Sng</td>\n",
       "      <td>2021-08-17</td>\n",
       "      <td>Can I tell you something I'm bad at? I am ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://youtube.com/watch?v=HeQX2HjkcNo</td>\n",
       "      <td>maths_fundamental_flaw</td>\n",
       "      <td>tiny</td>\n",
       "      <td>Math's Fundamental Flaw</td>\n",
       "      <td>Veritasium</td>\n",
       "      <td>21846330</td>\n",
       "      <td>2039</td>\n",
       "      <td>Not everything that is true can be proven. Thi...</td>\n",
       "      <td>HeQX2HjkcNo</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>There is a hole at the bottom of math. A hole...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       url  \\\n",
       "0  https://youtube.com/watch?v=GVsUOuSjvcg   \n",
       "1  https://youtube.com/watch?v=ieDIpgso4no   \n",
       "2  https://youtube.com/watch?v=Ek1buV2HA68   \n",
       "3  https://youtube.com/watch?v=ccO0_hSXMyM   \n",
       "4  https://youtube.com/watch?v=S2xHZPH5Sng   \n",
       "5  https://youtube.com/watch?v=HeQX2HjkcNo   \n",
       "\n",
       "                                            filename model_type  \\\n",
       "0  future_computers_will_be_radically_different_a...       tiny   \n",
       "1                       breaking_new_phase_of_matter       tiny   \n",
       "2                      worlds_only_moving_mud_puddle       tiny   \n",
       "3         how_mirrors_could_solve_our_energy_problem       tiny   \n",
       "4                clickbait_is_unreasonably_effective       tiny   \n",
       "5                             maths_fundamental_flaw       tiny   \n",
       "\n",
       "                                               title        author     views  \\\n",
       "0  Future Computers Will Be Radically Different (...    Veritasium   8672686   \n",
       "1                      BREAKING: New Phase of Matter  Physics Girl   1445568   \n",
       "2                     World's Only Moving Mud Puddle  Physics Girl   4100457   \n",
       "3         How Mirrors Could Solve our Energy Problem  Physics Girl   1072714   \n",
       "4                Clickbait is Unreasonably Effective    Veritasium   6098423   \n",
       "5                            Math's Fundamental Flaw    Veritasium  21846330   \n",
       "\n",
       "   length                                        description           id  \\\n",
       "0    1302  Visit https://brilliant.org/Veritasium/ to get...  GVsUOuSjvcg   \n",
       "1     934  What are time crystals? How do scientists make...  ieDIpgso4no   \n",
       "2     699  Want to support more videos like this? Patreon...  Ek1buV2HA68   \n",
       "3     690  We visited a giant field of solar mirrors to l...  ccO0_hSXMyM   \n",
       "4    1165  The title and thumbnail play a huge role in a ...  S2xHZPH5Sng   \n",
       "5    2039  Not everything that is true can be proven. Thi...  HeQX2HjkcNo   \n",
       "\n",
       "  publish_date                                 transcription_text  \n",
       "0   2022-03-01   For hundreds of years, analog computers were ...  \n",
       "1   2022-06-15   We're filming a video about time crystals. Ok...  \n",
       "2   2021-05-10   Hey, let's pull them straight in line in here...  \n",
       "3   2021-08-31   This is 4,000 acres of mirrors. As you drive ...  \n",
       "4   2021-08-17   Can I tell you something I'm bad at? I am ter...  \n",
       "5   2021-05-22   There is a hole at the bottom of math. A hole...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.data.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff92f38f-2598-494a-a8f5-f6763948c593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For hundreds of years, analog computers were the most powerful computers on Earth, predicting eclipses, tides, and guiding anti-aircraft guns. Then with the advent of solid state transistors, digital computers took off. Now virtually every computer we use is digital. But today, a perfect storm of factors is setting the scene for a resurgence of analog technology. This is an analog computer, and by connecting these wires in particular ways, I can program it to solve a whole range of differential equations. For example, this setup allows me to simulate a damped mass oscillating on a spring. So on the oscilloscope, you can actually see the position of the mass over time. I can vary the damping or the spring constant or the mass, and we can see how the amplitude and duration of the oscillations change. Now what makes this an analog computer is that there are no zeros and ones in here. Instead, there's actually a voltage that oscillates up and down exactly like a mass on a spring. The electrical circuitry is an analog for the physical problem. It just takes place much faster. Now if I change the electrical connections, I can program this computer to solve other differential equations like the Lorentz system, which is a basic model of convection in the atmosphere. Now the Lorentz system is famous because it was one of the first discovered examples of chaos. And here you can see the Lorentz attractor with its beautiful butterfly shape. And on this analog computer, I can change the parameters and see their effects in real time. So these examples illustrate some of the advantages of analog computers. They are incredibly powerful computing devices, and they can complete a lot of computations fast. Plus, they don't take much power to do it. With the digital computer, if you want to add two 8-bit numbers, you need around 50 transistors. Whereas with an analog computer, you can add two currents just by connecting two wires. With the digital computer to multiply two numbers, you need on the order of 1,000 transistors. All switching zeros and ones. Whereas with an analog computer, you can pass the current through a resistor. And then the voltage across this resistor will be i times r. So effectively, you have multiplied two numbers together. But analog computers also have their drawbacks. For one thing, they are not general purpose computing devices. I mean, you're not going to run Microsoft Word on this thing. And also, since the inputs and outputs are continuous, I can't input exact values. So if I try to repeat the same calculation, I'm never going to get the exact same answer. Plus, think about manufacturing analog computers. There's always going to be some variation in the exact value of components like resistors or capacitors. So as a general rule of thumb, you can expect about a 1% error. So when you think of analog computers, you can think powerful, fast, and energy efficient, but also single purpose non-repeatable and in exact. And if those sound like dealbreakers, it's because they probably are. I think these are the major reasons why analog computers fell out of favor. As soon as digital computers became viable. Now, here is why analog computers may be making a comeback. It all starts with artificial intelligence. A machine has been programmed to see and to move objects. AI isn't new. The term was coined back in 1956. In 1958, Cornell University psychologist Frank Rosenblatt built the perceptron, designed to mimic how neurons fire in our brains. So here's a basic model of how neurons in our brains work. An individual neuron can either fire or not. So its level of activation can be represented as a 1 or a 0. The input to one neuron is the output from a bunch of other neurons. But the strength of these connections between neurons varies. So each one can be given a different weight. Some connections are excitatory, so they have positive weights, while others are inhibitory, so they have negative weights. And the way to figure out whether a particular neuron fires is to take the activation of each input neuron and multiply by its weight. And then add these all together. If there's some is greater than some number, called the bias, then the neuron fires. But if it's less than that, the neuron doesn't fire. As input, Rosenblatt's perceptron had 400 photosiles arranged in a square grid to capture a 20 by 20 pixel image. You can think of each pixel as an input neuron with its activation being the brightness of the pixel. Although strictly speaking, the activation should be either 0 or 1, we can let it take any value between 0 and 1. All of these neurons are connected to a single output neuron. Each via its own adjustable weight. So to see if the output neuron will fire, you multiply the activation of each neuron by its weight and add them together. This is essentially a vector dot product. If the answer is larger than the bias, the neuron fires. And if not, it doesn't. Now the goal of the perceptron was to reliably distinguish between two images, like a rectangle and a circle. For example, the output neuron could always fire when presented with a circle, but never when presented with a rectangle. To achieve this, the perceptron had to be trained, that is shown a series of different circles and rectangles, and habits weights adjusted accordingly. We can visualize the weights as an image, since there's a unique weight for each pixel of the image. Initially, Rosenblatt set all the weights to 0. If the perceptron's output is correct, for example, here it's shown a rectangle and the output neuron doesn't fire. No changes made to the weights, but if it's wrong, then the weights are adjusted. The algorithm for updating the weights is remarkably simple. Here the output neuron didn't fire when it was supposed to, because it was shown a circle. So to modify the weights, you simply add the input activations to the weights. If the output neuron fires when it shouldn't, like here, when shown a rectangle, well then you subtract the input activations from the weights. And you keep doing this until the perceptron correctly identifies all the training images. It was shown that this algorithm will always converge, so long as it's possible to map the two categories into distinct groups. The perceptron was capable of distinguishing between different shapes, like rectangles and triangles, or between different letters. In according to Rosenblatt, it could even tell the difference between cats and dogs. He said the machine was capable of what amounts to original thought. And the media laughed it up. The New York Times called the perceptron the embryo of an electronic computer that the Navy expects will be able to walk, talk, see, write, reproduce itself, and be conscious of its existence. After training on lots of examples, it's given new faces it has never seen, and is able to successfully distinguish male from female. It has learned. In reality, the perceptron was pretty limited in what it could do. It could not, in fact, tell apart dogs from cats. This and other critiques were raised in a book by MIT Giants, Minsky and Pappert in 1969. And that led to a bus period for artificial neural networks and AI in general. It's known as the first AI winter. Rosenblatt did not survive this winter. He drowned while sailing in Chesapeake Bay on his 43rd birthday. The Now Lab is a road worthy truck, modified so that researchers or computers can control the vehicle as occasion demands. In the 1980s, there was an AI resurgence when researchers at Carnegie Mellon created one of the first self-driving cars. The vehicle was steered by an artificial neural network called Alvin. It was similar to the perceptron, except it had a hidden layer of artificial neurons between the input and output. As input, Alvin received 30 by 32 pixel images of the road ahead. Here, I'm showing them as 60 by 64 pixels. But each of these input neurons was connected via an adjustable weight to a hidden layer of four neurons. These were each connected to 32 output neurons. So to go from one layer of the network to the next, you perform a matrix multiplication. The input activation times the weights. The output neuron with the greatest activation determines the steering angle. To train the neural net, a human drove the vehicle, providing the correct steering angle for a given input image. All the weights in the neural network were adjusted through the training so that Alvin's output better matched that of the human driver. The method for adjusting the weights is called back propagation, which I won't go into here, but Welch Labs has a great series on this, which I'll link to in the description. Again, you can visualize the weights for the four hidden neurons as images. The weights are initially set to be random, but as training progresses, the computer learns to pick up on certain patterns. You can see the road markings emerge in the weights. Simultaneously, the output steering angle coalesces onto the human steering angle. The computer drove the vehicle at a top speed of around 1 or 2 kilometers per hour. It was limited by the speed at which the computer could perform matrix multiplication. Despite these advances, artificial neural networks still struggled with seemingly simple tasks, like telling apart cats and dogs. And no one knew whether hardware or software was the weak link. I mean, did we have a good model of intelligence? We just needed more computer power? Or did we have the wrong idea about how to make intelligent systems all together? So artificial intelligence experienced another law in the 1990s. By the mid-2000s, most AI researchers were focused on improving algorithms. But one researcher, Faye-Fay Lee, thought maybe there was a different problem. Maybe these artificial neural networks just needed more data to train on. So she planned to map out the entire world of objects. From 2006 to 2009, she created ImageNet, a database of 1.2 million human-labeled images, which at the time was the largest labeled Image Dataset ever constructed. And from 2010 to 2017, ImageNet ran an annual contest, the ImageNet large-scale visual recognition challenge, where software programs competed to correctly detect and classify images. Images were classified into 1,000 different categories, including 90 different dog breeds. The neural network, competing this competition, would have an output layer of 1,000 neurons. Each corresponding to a category of object that could appear in the image. If the image contains, say, a German shepherd, then the output neuron corresponding to German shepherd should have the highest activation. Unsurprisingly, it turned out to be a tough challenge. One way to judge the performance of an AI is to see how often the five highest neuron activations do not include the correct category. This is the so-called top five error rate. In 2010, the best performer had a top five error rate of 28.2%. Meaning that nearly a third of the time, the correct answer was not among its top five guesses. In 2011, the error rate of the best performer was 25.8% a substantial improvement. But the next year, an artificial neural network from the University of Toronto called AlexNet blew away the competition with a top five error rate of just 16.4%. What set AlexNet apart was its size and depth? The network consisted of eight layers and in total 500,000 neurons. To train AlexNet, 60 million weights and biases had to be carefully adjusted using the training database. Because of all the big matrix multiplications, processing a single image required 700 million individual math operations. So training was computationally intensive. The team managed it by pioneering the use of GPUs, graphical processing units, which are traditionally used for driving displays, screens. So they're specialized for fast parallel computations. The AlexNet paper describing their research is a blockbuster. It's now been cited over 100,000 times and it identifies the scale of the neural network as key to its success. It takes a lot of computation to train and run the network, but the improvement in performance is worth it. With others following their lead, the top five error rate on the image net competition plummeted in the years that followed down to 3.6% in 2015. That is better than human performance. The neural network that achieved this had 100 layers of neurons. So the future is clear. We will see ever increasing demand for ever larger neural networks. And this is a problem for several reasons. One is energy consumption. Training a neural network requires an amount of electricity similar to the yearly consumption of 3 households. Another issue is the so-called Von Neumann bottleneck, virtually every modern digital computer stores data in memory and then access it as needed over a bus. When performing the huge matrix multiplications required by deep neural networks, most of the time an energy goes into fetching those weight values rather than actually doing the computation. And finally, there are the limitations of Moore's law. For decades, the number of transistors on a chip has been doubling approximately every two years. But now the size of a transistor is approaching the size of an atom. So there are some fundamental physical challenges to further miniaturization. So this is the perfect storm for analog computers. Digital computers are reaching their limits. Meanwhile, neural networks are exploding in popularity. And a lot of what they do boils down to a single task, matrix multiplication. Best of all, neural networks don't need the precision of digital computers, whether the neural net is 96% or 98% confident the image contains a chicken. It doesn't really matter. It's still a chicken. So slight variability in components or conditions can be tolerated. I went to an analog computing startup in Texas called Mythic AI. Here they're creating analog chips to run neural networks. And they demonstrated several AI algorithms for me. Oh, there you go. See, it's getting you. Yeah, that's fascinating. The biggest use case is augmented in virtual reality. You know, if your friend is in a different, you know, they're at their house and you're at your house. You can actually render each other in the virtual world. So it needs to really quickly capture your pose and then render it in the VR world. So what I'm going to do is this for the Metaverse. This is very metaverse application. This is depth estimation from just a single webcam. It's just taking this scene and then it's doing like a heat map. So if it's bright, it means it's close. If it's far away, it makes it like black. Now all these algorithms can be run on digital computers. But here, the matrix multiplication is actually taking place in the analog domain. To make this possible, Mythic has repurposed digital flash storage cells. Normally, these are used as memory to store either a 1 or a 0. If you apply a large positive voltage to the control gate, electrons tunnel up through an insulating barrier and become trapped on the floating gate. Remove the voltage and the electrons can remain on the floating gate for decades, preventing the cell from conducting current. That's how you can store either a 1 or a 0. You can read out the stored value by applying a small voltage. If there are electrons on the floating gate, no current flows. So that's a 0. If there aren't electrons, then current does flow and that's a 1. Now, Mythic's idea is to use these cells not as on-off switches, but as variable resistors. They do this by putting a specific number of electrons on each floating gate instead of all or nothing. The greater the number of electrons, the higher the resistance of the channel. When you later apply a small voltage, the current that flows is equal to v over r. But you can also think of this as voltage times conductance, where conductance is just the reciprocal of resistance. So a single flash cell can be used to multiply two values together, voltage times conductance. So to use this to run an artificial neural network, they first write all the weights to the flash cells as each cell's conductance. Then they input the activation values as the voltage on the cells. And the resulting current is the product of voltage times conductance, which is activation times weight. The cells are wired together in such a way that the current from each multiplication adds together, completing the matrix multiplication. So this is our first product. This can do 25 trillion math operations per second. 25 trillion? Yeah, 25 trillion math operations per second. And this little chip here burning about three watts a power. How does it compare to a digital chip? The newer digital systems can do anywhere from 25 to 100 trillion operations per second. But they are big thousand dollar systems that are spitting out 50 to 100 watts a power. Obviously, this isn't an apples to apples come to earth. It's not apples to apples. I mean, training those algorithms, you need big hardware like this. And you can just do all sorts of stuff on the GPU. But if you specifically are doing AI workloads and you wanted to deploy them, you could use this instead. You can imagine them in security cameras, autonomous systems, inspection equipment for manufacturing. Every time they make a free-to-lay chip, they inspect it with a camera. And the bad free-dose get blown off of the conveyor belt. But they're using artificial intelligence to spot which free-dose are good and bad. Some have proposed using analog circuitry in smart home speakers solely to listen for the wakeward, like Alexa or Siri. They would use a lot less power and be able to quickly and reliably turn on the digital circuitry of the device. But you still have to deal with the challenges of analog. So for one of the popular networks, there would be 50 sequences of matrix multiplies that you're doing. Now, if you did that entirely in the analog domain, by the time it gets to the output, it's just so distorted that you don't have any resulted all. So you convert it from the analog domain back to the digital domain, send it to the next processing block, and then you convert it into the analog domain again. And that allows you to preserve the signal. You know, when Rosenblatt was first setting up his perceptron, he used a digital IBM computer. Finding it too slow, he built a custom analog computer, complete with variable resistors and little motors to drive them. You know, ultimately his idea of neural networks turned out to be right. Maybe he was right about analog too. Now, I can't say whether analog computers will take off the way digital did last century, but they do seem to be better suited to a lot of the tasks that we want computers to perform today, which is a little bit funny because I always thought of digital as the optimal way of processing information. You know, everything from music to pictures to video has all gone digital in the last 50 years. But maybe in a hundred years, we will look back on digital, not as the end point of information technology, but as a starting point, our brains are digital in that a neuron either fires or it doesn't, but they're also analog in it, thinking takes place everywhere, all at once. So maybe what we need to achieve true artificial intelligence, machines that think like us, is the power of analog. Okay. Hey, I learned a lot while making this video, much of it by playing with an actual analog computer. You know, trying things out for yourself is really the best way to learn, and you can do that with this video sponsor, Brilliant. Brilliant is a website and app that gets you thinking deeply by engaging you in problem solving. They have a great course on neural networks, where you can test how it works for yourself. It gives you an excellent intuition about how neural networks can recognize numbers and shapes, and it also allows you to experience the importance of good training data and hidden layers to understand why more sophisticated neural networks work better. What I love about Brilliant is it tests your knowledge as you go. The lessons are highly interactive and they get progressively harder as you go on, and if you get stuck, there are always helpful hints. For viewers of this video, Brilliant is offering the first 200 people 20% off an annual premium subscription, just go to Brilliant.org slash baritassium. I will put that link down in the description. So I want to thank Brilliant for supporting baritassium, and I want to thank you for watching.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.data.results.transcription_text.values[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d2cbd-1bd2-4856-b515-4b6c9122ea56",
   "metadata": {},
   "source": [
    "# Doing Random Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd65022-c99d-4565-9e55-9467b84aac89",
   "metadata": {},
   "source": [
    "## Grab a YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542363f2-28ab-4bce-9a18-773e5d720a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d9a978c-911d-4ddf-847e-c459b0c95d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = YouTube('https://youtube.com/watch?v=hIiDXPHqEFM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29876557-84d3-4561-9e9a-dcd9db6faa4d",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* `base` model on remote instances produced `RuntimeError: Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.` [link](https://github.com/openai/whisper/blob/82725cea9c339cdc3a2004a622bba766b1871945/whisper/__init__.py#L58). Error was on batch cpu compute environment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
